From the ca-vm SSH shell, enter the following command to install the Kubernetes package from the Red Hat Enterprise Linux (RHEL) package repository:
-----------------

sudo yum install -y kubernete

Enter the following to install etcd:


sudo yum install -y etcd
. Enter ls /etc/kubernetes to view the configuration files used for each Kubernetes component

In the common config file, change the KUBE_MASTER API server address to use the control plane's DNS name:


sudo sed -i 's/\(KUBE_MASTER.*\)127.0.0.1\(.*\)/\1ca-lab-vm\2/g' /etc/kubernetes/config
This command replaces the default control plane host of 127.0.0.1 with ca-vm. Within the virtual network, the server can be addressed by ca-vm. In the command, the \1 and \2 before and after ca-vm expand to everything that is before and after 127.0.0.1 in the original line. This has the effect of only changing 127.0.0.1 to ca-vm in the line. 

In the apiserver configuration file, change the bind address to listen on all interfaces (0.0.0.0):

sudo sed -i 's/\(KUBE_API_ADDRESS.*\)127.0.0.1\(.*\)/\10.0.0.0\2/g' /etc/kubernetes/apiserver
This allows other nodes to communicate with the Kubernetes API.

 Create a key to authorize API requests from pods:

openssl genrsa -out /tmp/serviceaccount.key 2048
For convenience, it is stored in /tmp. In a production environment, you would protect the key better

A specific controller called the admissions controller is responsible for authorizing API server requests. A service account is used to identify processes that run in pods. A service account called default is automatically created for this purpose. The key you created will be used to authorize the default service account requests.

Specify the service account key to use for the API server and the controller manager:


sudo sed -i 's/\(KUBE_API_ARGS=\).*/\1"--service_account_key_file=\/tmp\/serviceaccount.key"/' /etc/kubernetes/apiserver
sudo sed -i 's/\(KUBE_CONTROLLER_MANAGER_ARGS=\).*/\1"--service_account_private_key_file=\/tmp\/serviceaccount.key"/' /etc/kubernetes/controller-manager


Enter the following to configure etcd to also listen on all interfaces:


sudo sed -i 's/\(ETCD_LISTEN_CLIENT_URLS.*\)localhost\(.*\)/\10.0.0.0\2/g' /etc/etcd/etcd.conf

Start, enable the services on reboot, and list the status of the services required by the control plane:


for SERVICE in etcd kube-apiserver kube-controller-manager kube-scheduler; do
    sudo systemctl restart $SERVICE
    sudo systemctl enable $SERVICE
    sudo systemctl status $SERVICE
done

 Verify the Kubernetes API server by issuing an HTTP GET request to return all the API groups supported by the server:


curl http://ca-vm:8080/apis | more

Enter the following command to show the versions of supported API groups:


kubectl api-versions

nter the following command to re-run the last command with verbose output:


kubectl api-versions --v=7

Create the worker node resource:


kubectl create -f node.yml

 Enter the following command to display all the node resources in the cluster along with their labels:


kubectl get nodes --show-labels

In order for the worker node to be able to connect to the kube-apiserver on the control plane node,will need to open allow the connection:


sudo systemctl stop firewalld
This stops the operating system's firewall all together. 

From the k8s-node SSH shell, enter the following command to install the Kubernetes package from the Red Hat Enterprise Linux (RHEL) package repository:

sudo yum install -y kubernetes

In the Kubernetes common config file, change the KUBE_MASTER API server address to use the control plane's DNS name:

sudo sed -i 's/\(KUBE_MASTER.*\)127.0.0.1\(.*\)/\1ca-vm\2/g' /etc/kubernetes/config
This is the same modification made to the control plane node config file. All servers in a cluster should have the same config file.

Enter the following commands to configure the kubelet:


# Listen on all interfaces
sudo sed -i 's/\(KUBELET_ADDRESS.*\)127.0.0.1\(.*\)/\10.0.0.0\2/g' /etc/kubernetes/kubelet
# Use the DNS hostname for the node
sudo sed -i 's/\(KUBELET_HOSTNAME.*\)127.0.0.1\(.*\)/\1k8s-node\2/g' /etc/kubernetes/kubelet
# Use the control plane's DNS name for the API server address
sudo sed -i 's/\(KUBELET_API_SERVER.*\)127.0.0.1\(.*\)/\1ca-lab-vm\2/g' /etc/kubernetes/kubelet

Start, enable on reboot, and show the status of the required worker node services:

for SERVICE in kube-proxy kubelet docker; do 
    sudo systemctl restart $SERVICE
    sudo systemctl enable $SERVICE
    sudo systemctl status $SERVICE 
done
Return to your ca-vm SSH session, and re-run the following command:

kubectl get nodes

Create the deployment resource specified in the deployment.yml file:


kubectl create -f deployment.yml
 
Show more details about the deployment:


kubectl describe deployment game-deployment
In the event of failed deployments, can use this command to understand the reason the deployment failed. Under the Conditions section, only the Available condition is present:

view the cluster-wide events that have been recorded by the cluster:


kubectl get events | more

Get the pod resource created by the deployment:

kubectl get pods

Describe the pod that is running the container in the deployment:


kubectl describe pods | more
 can see a variety of details including Containers, and Events for the pod. You can append a specific pod name to the end of the command to describe a specific pod. In this case, there is only one pod, so there is no need to specify the name.

Describe all the pods with the label app=game: 

kubectl get pods -l app=game
Recall that the app: game label is specified in the template metadata in the deployment.yml file. Each pod created by the deployment is assigned this label. Labels can be used to filter the output of many commands. 

Create the service:


kubectl create -f service.yml

Describe the service and record what NodePort was allocated:

kubectl describe services game

 enter curl ipinfo.io/ip to get the worker node's public IP address.

Disable the operating system firewall to allow external connections:

sudo systemctl stop firewalld

 allow the required traffic through the operating system firewall in production. The Azure network security group has been configured to allow traffic on the game port already.

  Open a web browser and enter the node's public IP address followed by a : and the NodePort you recorded. The full address will resemble the address in the image below:

Return to the ca-vm control plane node SSH session, and scale the application by changing the deployment replicas count to 2:

sed -i 's/\(replicas: \).*/\12/' deployment.yml

Apply the new resource:

kubectl apply -f deployment.yml

Verify that the number of pods increases to 2:


kubectl get pods -l app=game
 
 
Delete the game service:


kubectl delete service game
 

2. Delete the game deployment:


kubectl delete deployment game-deployment
 

3. Observe that the pods are automatically deleted with the deployment deletion:


kubectl get pods
 

4. Gracefully terminate the worker node:


kubectl drain k8s-node
The drain subcommand marks the node as unscheduable so no new pods can be assigned to it (the same as calling the cordon subcommand) and waits for graceful termination of the pods running on the node:

kubectl get node

Delete the worker node from the cluster:


kubectl delete node k8s-node
